{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InterpretableSAD_HDFS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm6OkgVsbafJ"
      },
      "source": [
        "# Install captum package\r\n",
        "!pip install captum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J9A0V63qLeW"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from utils import preprocessing, SlidingWindow, NegativeSampling, utils, modelhdfs, explainhdfs\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from tqdm import tqdm\r\n",
        "import time\r\n",
        "import math\r\n",
        "import os\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import precision_recall_fscore_support\r\n",
        "from captum.attr import LayerIntegratedGradients\r\n",
        "import collections"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7vbfwlP8IfJ"
      },
      "source": [
        "DATASET_NAME = 'HDFS'\r\n",
        "TRAIN_SIZE = 100000\r\n",
        "RATIO = 0.1\r\n",
        "SEED = 42"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yerXXxqJqetY"
      },
      "source": [
        "# Download dataset and parsing the dataset with Drain\r\n",
        "preprocessing.parsing(DATASET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERsjmU1r5Zo9"
      },
      "source": [
        "# Cut log data into sliding windows\r\n",
        "# Split data into training normal dataset, test normal dataset, and test abnormal dataset\r\n",
        "# Get the bigram from training normal dataset\r\n",
        "# Train a Word2Vec model with the training normal data\r\n",
        "# Number of keys include 'pad'\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "train_normal, test_normal, test_abnormal, bigram, unique, weights, train_dict, w2v_dic = SlidingWindow.sliding(DATASET_NAME, train_size=TRAIN_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsRoGViaEpG"
      },
      "source": [
        "# +1 for unknown\r\n",
        "VOCAB_DIM = len(train_dict)+1\r\n",
        "OUTPUT_DIM = 2\r\n",
        "EMB_DIM = 4\r\n",
        "HID_DIM = 64\r\n",
        "N_LAYERS = 1\r\n",
        "DROPOUT = 0.0\r\n",
        "BATCH_SIZE = 32\r\n",
        "TIMES = 20"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8HIfgwvblO5"
      },
      "source": [
        "# Get negative samples and split into training data and val data\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "neg_samples = NegativeSampling.negative_sampling_hdfs(train_normal, bigram, unique, TIMES, VOCAB_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEk54oerMa5"
      },
      "source": [
        "df_neg = utils.get_dataframe(neg_samples, 1, w2v_dic)\r\n",
        "df_pos = utils.get_dataframe(list(train_normal['EventSequence']), 0, w2v_dic)\r\n",
        "df_pos.columns = df_pos.columns.astype(str)\r\n",
        "df_train = pd.concat([df_pos, df_neg], ignore_index = True, axis=0)\r\n",
        "df_train.reset_index(drop = True)\r\n",
        "y = list(df_train.loc[:,'class_label'])\r\n",
        "X = list(df_train['W2V_EventId'])\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
        "train_iter = utils.get_iter_hdfs(X_train, y_train, w2v_dic, train_dict, BATCH_SIZE)\r\n",
        "val_iter = utils.get_iter_hdfs(X_val, y_val, w2v_dic, train_dict, BATCH_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNSIPEQ3vqwx"
      },
      "source": [
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "device = torch.device( \"cuda\" if torch.cuda.is_available() else\"cpu\")\r\n",
        "interpretableSAD = modelhdfs.C_lstm(weights, VOCAB_DIM, OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT, device, BATCH_SIZE).to(device)\r\n",
        "print(f'The model has {modelhdfs.count_parameters(interpretableSAD):,} trainable parameters')\r\n",
        "print()\r\n",
        "optimizer = optim.Adam(interpretableSAD.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "try:\r\n",
        "    os.makedirs('Model')\r\n",
        "except:\r\n",
        "    pass\r\n",
        "\r\n",
        "#Training interpretableSAD\r\n",
        "N_EPOCHS = 1\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_test_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in tqdm(range(N_EPOCHS)):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    train_loss= modelhdfs.train(interpretableSAD, train_iter, optimizer, criterion, CLIP, epoch, device)        \r\n",
        "\r\n",
        "    val_loss = modelhdfs.evaluate(interpretableSAD, val_iter, criterion, device)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = modelhdfs.epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if val_loss < best_test_loss:\r\n",
        "        best_test_loss = val_loss\r\n",
        "        torch.save(interpretableSAD.state_dict(), 'Model/interpretableSAD_HDFS.pt')\r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo2CgwMCvu6R"
      },
      "source": [
        "test_ab_X = test_abnormal['W2V_EventId']\r\n",
        "test_n_X = test_normal['W2V_EventId']\r\n",
        "y, y_pre = modelhdfs.model_precision(interpretableSAD, device, w2v_dic, train_dict, test_n_X.values.tolist(), test_ab_X.values.tolist())\r\n",
        "f1_acc = metrics.classification_report(y, y_pre, digits=5)\r\n",
        "print(f1_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu0LfXrvcBub"
      },
      "source": [
        "lig = LayerIntegratedGradients(interpretableSAD, interpretableSAD.embedding)\r\n",
        "lst_train_keys = []\r\n",
        "for i in train_normal.W2V_EventId.values:\r\n",
        "    lst_train_keys.extend(i)\r\n",
        "dic_app = collections.Counter(lst_train_keys)\r\n",
        "if w2v_dic[str(len(train_dict))] not in dic_app.keys():\r\n",
        "    dic_app[w2v_dic[str(len(train_dict))]] = 0\r\n",
        "start = [w2v_dic[i] for i in unique]\r\n",
        "df_attr = explainhdfs.get_dataset(interpretableSAD, device, lig, test_abnormal, dic_app, start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoEGTpBZd09H"
      },
      "source": [
        "%%capture cap\r\n",
        "for i in range(len(df_attr)):\r\n",
        "    if len(df_attr['Event'].iloc[i]) >10 and len(df_attr['Event'].iloc[i]) < 30:\r\n",
        "        display(explainhdfs.visualize_token_attrs(df_attr['Event'].iloc[i], np.array(df_attr['Attr'].iloc[i]), df_attr['Blk'].iloc[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PojhVWv87sFN"
      },
      "source": [
        "cap()"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}